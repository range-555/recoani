{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import MeCab\n",
    "import mysql.connector as mydb\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mydb.connect(\n",
    "    host = os.environ.get('ml_db_host'),\n",
    "    port = os.environ.get('local_port'),\n",
    "    user = os.environ.get('recoani_user'),\n",
    "    password = os.environ.get('recoani_pass'),\n",
    "    database = os.environ.get('recoani_db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pos = {\n",
    "    \"助詞,格助詞,一般,*\",\n",
    "    \"助詞,格助詞,引用,*\",\n",
    "    \"助詞,格助詞,連語,*\",\n",
    "    \"助詞,係助詞,*,*\",\n",
    "    \"助詞,終助詞,*,*\",\n",
    "    \"助詞,接続助詞,*,*\",\n",
    "    \"助詞,特殊,*,*\",\n",
    "    \"助詞,副詞化,*,*\",\n",
    "    \"助詞,副助詞,*,*\",\n",
    "    \"助詞,副助詞／並立助詞／終助詞,*,*\",\n",
    "    \"助詞,並立助詞,*,*\",\n",
    "    \"助詞,連体化,*,*\",\n",
    "    \"助動詞,*,*,*\",\n",
    "    \"記号,句点,*,*\",\n",
    "    \"記号,読点,*,*\",\n",
    "    \"記号,空白,*,*\",\n",
    "    \"記号,一般,*,*\",\n",
    "    \"記号,アルファベット,*,*\",\n",
    "    \"記号,一般,*,*\",\n",
    "    \"記号,括弧開,*,*\",\n",
    "    \"記号,括弧閉,*,*\",\n",
    "    \"動詞,接尾,*,*\",\n",
    "    \"動詞,非自立,*,*\",\n",
    "    \"名詞,非自立,一般,*\",\n",
    "    \"名詞,非自立,形容動詞語幹,*\",\n",
    "    \"名詞,非自立,助動詞語幹,*\",\n",
    "    \"名詞,非自立,副詞可能,*\",\n",
    "    \"名詞,接尾,助動詞語幹,*\",\n",
    "    \"名詞,接尾,人名,*\",\n",
    "    \"接頭詞,名詞接続,*,*\"\n",
    "    }\n",
    "stop_words = []\n",
    "stop_word_regex = []\n",
    "vocab_list = []\n",
    "get_word = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章を正規化するクラス\n",
    "class Normalize:\n",
    "    \n",
    "    def __init__(self,sentence):\n",
    "        self.sentence = sentence\n",
    "        \n",
    "    def normalize(self):\n",
    "        return self.normalize_neologd()\n",
    "\n",
    "    def unicode_normalize(self,cls,s):\n",
    "        pt = re.compile('([{}]+)'.format(cls))\n",
    "\n",
    "        def norm(c):\n",
    "            return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
    "    \n",
    "        s = ''.join(norm(x) for x in re.split(pt, s))\n",
    "        s = re.sub('－', '-', s)\n",
    "        return s\n",
    "\n",
    "    def remove_extra_spaces(self,s):\n",
    "        s = re.sub('[ 　]+', ' ', s)\n",
    "        blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
    "                          '\\u3040-\\u309F',  # HIRAGANA\n",
    "                          '\\u30A0-\\u30FF',  # KATAKANA\n",
    "                          '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
    "                          '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
    "                          ))\n",
    "        basic_latin = '\\u0000-\\u007F'\n",
    "    \n",
    "        def remove_space_between(cls1,cls2,s):\n",
    "            p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
    "            while p.search(s):\n",
    "                s = p.sub(r'\\1\\2', s)\n",
    "            return s\n",
    "    \n",
    "        s = remove_space_between(blocks, blocks, s)\n",
    "        s = remove_space_between(blocks, basic_latin, s)\n",
    "        s = remove_space_between(basic_latin, blocks, s)\n",
    "        return s\n",
    "    \n",
    "    def normalize_neologd(self):\n",
    "        s = self.sentence\n",
    "        s = s.strip()\n",
    "        s = self.unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
    "        def maketrans(f, t):\n",
    "            return {ord(x): ord(y) for x, y in zip(f, t)}\n",
    "    \n",
    "        s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
    "        s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
    "        s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
    "        s = s.translate(\n",
    "            maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
    "                  '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
    "        s = self.remove_extra_spaces(s)\n",
    "        s = self.unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
    "        s = re.sub('[’]', '\\'', s)\n",
    "        s = re.sub('[”]', '\"', s)\n",
    "        s = s.upper()\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素解析用クラス\n",
    "class Morph(object):\n",
    "    def __init__(self, surface, pos, base):\n",
    "        self.surface = surface\n",
    "        self.pos = pos\n",
    "        self.base = base\n",
    "    def __repr__(self):\n",
    "        return str({\n",
    "            \"surface\": self.surface,\n",
    "            \"pos\": self.pos,\n",
    "            \"base\": self.base\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence,title):\n",
    "    if len(sentence) < 10:\n",
    "        sentence = title\n",
    "    \n",
    "    s = Normalize(sentence)\n",
    "    sentence = s.normalize()\n",
    "    mecab.parse(\"\")\n",
    "    lines = mecab.parse(sentence).split(\"\\n\")\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        elems = line.split(\"\\t\")\n",
    "        if len(elems) < 2:\n",
    "            continue\n",
    "        surface = elems[0]\n",
    "        if len(surface):\n",
    "            feature = elems[1].split(\",\")\n",
    "            base = surface if len(feature) < 7 or feature[6] == \"*\" else feature[6]\n",
    "            pos = \",\".join(feature[0:4])\n",
    "            tokens.append(Morph(surface=surface, pos=pos, base=base))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_list(data):\n",
    "    vocab = {}\n",
    "    for item in data:\n",
    "        tokens = item['tokens']\n",
    "        for token in tokens:\n",
    "            key = token.base\n",
    "            pos = token.pos\n",
    "            is_stop = pos in stop_pos\n",
    "            v = vocab.get(key, { \"count\": 0, \"pos\": pos , \"stop\": is_stop})\n",
    "            v[\"count\"] += 1\n",
    "            vocab[key] = v\n",
    "    for k in vocab:\n",
    "        v = vocab[k]\n",
    "        if not v[\"stop\"]:\n",
    "            vocab_list.append((v[\"count\"], k, v[\"pos\"], v[\"stop\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop(vocab):\n",
    "    return vocab[2] in stop_pos or vocab[1] in stop_words or any([r for r in stop_word_regex if r.match(vocab[1]) is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=4,\n",
    "    max_features=1280,\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor(buffered=True, dictionary=True)\n",
    "cursor.execute(\"SELECT id, title, outline_entire FROM animes\")\n",
    "data = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    item['tokens'] = tokenize(item['outline_entire'],item['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vocab_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8890, 'する', '動詞,自立,*,*', False)\n",
      "(3685, 'いる', '動詞,自立,*,*', False)\n",
      "(2274, 'なる', '動詞,自立,*,*', False)\n",
      "(2218, 'ある', '連体詞,*,*,*', False)\n",
      "(2208, 'ない', '形容詞,自立,*,*', False)\n",
      "(1996, 'たち', '名詞,接尾,一般,*', False)\n",
      "(1765, 'その', '連体詞,*,*,*', False)\n",
      "(1160, 'そして', '接続詞,*,*,*', False)\n",
      "(1033, 'ー', '名詞,一般,*,*', False)\n",
      "(987, '世界', '名詞,一般,*,*', False)\n"
     ]
    }
   ],
   "source": [
    "vocab_list = sorted(vocab_list, reverse=True)\n",
    "for i in range(10):\n",
    "    print(vocab_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count > 500 && stop_word\n",
    "stop_words += [\n",
    "    'する', 'いる', 'ある', 'たち',\n",
    "    'ない', 'なる', '人', 'その', '(', '\"',\n",
    "    ')', '.', '/', 'ー','そして', '年', '中',\n",
    "    'そんな', '一','2', '二', 'それ', 'この',\n",
    "    '1', '3', '第', 'できる'\n",
    "]\n",
    "stop_words += [\n",
    "    '監督','アニメーション','メンバー','エピソード','スタッフ',''\n",
    "]\n",
    "stop_word_regex = [ re.compile(\"^[!?]+$\") ]\n",
    "\n",
    "get_word = [v[1] for v in vocab_list if v[0] > 3 and not is_stop(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = {'id': [],'title': [] ,'outline': []}\n",
    "\n",
    "for item in data:\n",
    "    items['id'].append(item['id'])\n",
    "    items['title'].append(item['title'])\n",
    "    base = []\n",
    "    for token in item[\"tokens\"]:\n",
    "        if token.base not in get_word:\n",
    "            continue\n",
    "        base.append(token.base)\n",
    "    items['outline'].append(' '.join(base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_fit = tfidf.fit(items['outline'])\n",
    "tfidf_transform = tfidf.transform(items['outline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3901, 1280)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(tfidf_transform, tfidf_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(target):\n",
    "    cursor.execute(\"SELECT id FROM animes where title = %s\", [target])\n",
    "    target_data = cursor.fetchone()\n",
    "    target_id = target_data[\"id\"]\n",
    "    print(target_id)\n",
    "    sim_items_idx = cos_sim_ex[target_id].argsort()[::-1][:20]\n",
    "    print(\"タイトル \" + items['title'][target_id])\n",
    "    print(items['outline'][target_id])\n",
    "    print(\"id \" + str(items[\"id\"][target_id]))\n",
    "    for idx in sim_items_idx[1:]:\n",
    "        print('------------------------------------')\n",
    "        # 1000番目のデータと類似するデータの先頭60文字を表示します\n",
    "        print(items['title'][idx])\n",
    "        print(items['outline'][idx])\n",
    "        print(items[\"id\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2483\n",
      "タイトル のんのんびより りぴーと\n",
      "旭丘 分校 生徒 たった 5人 学年 性格 違う けれど 川 遊ぶ 作る いつも 一緒に 変わる 田舎 生活 楽しむ 穏やか 何気ない けれど 笑える 心 あたたかい ゆるやか やさしい 時間 再び はじまる\n",
      "id 2484\n",
      "------------------------------------\n",
      "劇場版 のんのんびより ばけーしょん\n",
      "旭丘 分校 生徒 たった 5人 学年 性格 違う けれど いつも 一緒に 変わる 田舎 生活 楽しむ ある日 デパート 沖縄 当てる 旭丘 分校 面々 夏休み 利用 皆 沖縄 行く …。 穏やか 何気ない けれど 笑える 心 あたたかい まったり ゆるゆる 送る 日常 日常 会意 来る\n",
      "2485\n",
      "------------------------------------\n",
      "カブキブ！\n",
      "歌舞伎 大好き 高校1年生 黒 夢 部活 歌舞伎 けれど 入学 高校 部 存在 自分たち 作る 親友 一緒に 集め 奔走 けれど 青春 歌舞伎 物語 開幕\n",
      "650\n",
      "------------------------------------\n",
      "劇場版「空の境界」第二章 殺人考察（前）\n",
      "私 おまえ 犯す 春 君 見つける 声 かける 返す やがて 君 ほんの少しだけ 僕 言葉 交わす けれど 君 誰 共有 秘密 抑える これ 高校生 16歳 儀式 出会う 物語 あえて 他者 交わる しない 式 どう しよう 惹く 幹也 なにか 関わる 恋 気付く やがて わずか 同じ 空間 共有 彼ら 確か 平穏 街 連続 猟奇殺人 事件 起こる けれど ある日 幹也 式 内 存在 もう ひとり 織る 人格 出会う 式 否定 織る けれど 同じ 思考 持つ 一人 幹也 徐々 乱す かくして 幹也 ひとつ 予感 胸 抱く 猟奇殺人 繰り返す 誰 けれど 予感 最初 己 見る 姿 信じる 出来る だからこそ 真実 確かめる 彼 密か 決意 スルー けれど 考察 真実 たどり着く 3年後 家 跡取り 条件 知る 関係 血 まり 佇む 少女 導く 運命 式 幹也 軌跡 幕開け 描く 章 殺人 考察 前\n",
      "676\n",
      "------------------------------------\n",
      "のんのんびより\n",
      "全校 生徒 たった 5人 旭丘 分校 春 足音 聞こえる 相変わらず まったり 過ごす 少女 魚 彼女たち 新しい 季節 楽しむ 方 触れる 子供 懐かしさ 発見 しれる\n",
      "2483\n",
      "------------------------------------\n",
      "甘々と稲妻\n",
      "一人娘 二人 暮らす ふとした きっかけ 女子高生 三 ごはん 作る 食べる 三 料理 まったく けれど 美味しい ごはん とっても 大好き! 楽しい ひととき きっと あなた 夢中\n",
      "169\n",
      "------------------------------------\n",
      "こねこのチー　ポンポンらー大冒険\n",
      "やんちゃ 子猫 チー 小さな 体 いつも 元気いっぱい やさしい 山田 一家 一緒に 楽しい 暮らす 大好き ミルク 飲む のんびり 昼寝 公園 散歩 大好き 愉快 猫 友だち たまに ケンカ けれど すぐ 何気ない 毎日 子猫 目 見る ワクワク ドキドキ 連続 大冒険\n",
      "1160\n",
      "------------------------------------\n",
      "おしえて！ ギャル子ちゃん\n",
      "見た目 どこ どう 見る 金髪 ギャル 遊ぶ 見える けれど 実は 純情 女の子 ギャル 外見 素直 優しい 内面 ギャップ 可愛らしい 中心 背 小さな おっとり 天真爛漫 なお 嬢 三 いろんな 場所 繰り広げる 日常 コメディ\n",
      "486\n",
      "------------------------------------\n",
      "ミュージカル「ヘタリア～in the new world～」東京大千秋楽版\n",
      "永い 戦い 終わる 別々 道 歩む それぞれ 地 目指す ドイツ 日本 いつも 変わる 元気 イタリア しかし 三 心 以前 違う すれ違う ある日 記 存在 バラバラ 心 再び かつて 列強 戦う 男 日記 …。 2017年 映像\n",
      "3100\n",
      "------------------------------------\n",
      "ミュージカル「ヘタリア～in the new world～」大阪千秋楽ノーカット版\n",
      "永い 戦い 終わる 別々 道 歩む それぞれ 地 目指す ドイツ 日本 いつも 変わる 元気 イタリア しかし 三 心 以前 違う すれ違う ある日 記 存在 バラバラ 心 再び かつて 列強 戦う 男 日記 …。 2017年 映像\n",
      "3099\n",
      "------------------------------------\n",
      "旦那が何を言っているかわからない件 2スレ目\n",
      "ごく 普通 OL カオル 彼女 旦那 重度 オタク ハジメ ラブ 生活 返る きた カオル 相変わらず ハジメ 振り回す けれど 見る 恥ずかしい 仲 良さ …。\n",
      "1936\n",
      "------------------------------------\n",
      "コンビニカレシ \n",
      "春 始まり 季節 親友 塔 羽 青 あおい そら 高校 入学 互いに 気 女の子 恋 始まる 気配 不器用 日々 過ごす 同級生 佐々木 先輩 夏 中島 帝 ら 出会う それぞれ 想い 絡み合う 誰 度 通る 道 瞬間 爽やか 駆け抜ける いつも 電車 いつも 通学 路 いつも コンビニ いつも 変わる 風景 明日 少し 違う 見える しれる\n",
      "1201\n",
      "------------------------------------\n",
      "TVアニメ「ペルソナ４ ザ・ゴールデン」\n",
      "春 都会 離れる 穏やか 時間 流れる 田舎町 桜 舞う 散る 八 十 羽 駅 降り立つ ひとり 少年 家庭 事情 叔父 町 やってくる 悠 地元 高校 転入 はじまる 学園 生活 放課後 街 起こる 連続殺人事件 深夜 流れる 悠 仲間達 今度 一体 どんな 日々 待ち受ける\n",
      "3125\n",
      "------------------------------------\n",
      "きんぎょ注意報！\n",
      "父親 死 貧乏 幸運 呼ぶ ピンク ちゃん 連れる 都会 学園 田舎 中学校 転校 そこ 千 歳 想像 絶する 世界 元気 少女 葵 クラスメイト ほか ブタ 同じ 教室 勉強 そのうえ 廃校 知らす しかし 父親 千 歳 莫大 遺産 遺産 千歳 田舎 中学校 建てる 理事長 兼 生徒会長 都会 学園 生徒会長 お嬢様 生活 パワー 押す 田舎 生活 ついつい ギャグ 顔 ギャグ\n",
      "898\n",
      "------------------------------------\n",
      "銀魂’\n",
      "天人 宇宙人 来襲 価値観 変わる 町 江戸 宇宙人 高層ビル バイク 何 世界 変わる 魂 持つ 最後 サムライ 男 名 坂田銀時 いい加減 無鉄砲 キメる さりげなく キメる 笑える 泣ける 心温まる 銀さん 仲間達 生き様 得 ご覧\n",
      "971\n",
      "------------------------------------\n",
      "銀魂.\n",
      "天人 宇宙人 来襲 価値観 変わる 町 江戸 宇宙人 高層ビル バイク 何 世界 変わる 魂 持つ 最後 サムライ 男 名 坂田銀時 いい加減 無鉄砲 キメる さりげなく キメる 笑える 泣ける 心温まる 銀さん 仲間達 生き様 得 ご覧\n",
      "974\n",
      "------------------------------------\n",
      "銀魂 (1年目)\n",
      "天人 宇宙人 来襲 価値観 変わる 町 江戸 宇宙人 高層ビル バイク 何 世界 変わる 魂 持つ 最後 サムライ 男 名 坂田銀時 いい加減 無鉄砲 キメる さりげなく キメる 笑える 泣ける 心温まる 銀さん 仲間達 生き様 得 ご覧\n",
      "967\n",
      "------------------------------------\n",
      "銀魂゜\n",
      "天人 宇宙人 来襲 価値観 変わる 町 江戸 宇宙人 高層ビル バイク 何 世界 変わる 魂 持つ 最後 サムライ 男 名 坂田銀時 いい加減 無鉄砲 キメる さりげなく キメる 笑える 泣ける 心温まる 銀さん 仲間達 生き様 得 ご覧\n",
      "973\n",
      "------------------------------------\n",
      "銀魂 (2年目)\n",
      "天人 宇宙人 来襲 価値観 変わる 町 江戸 宇宙人 高層ビル バイク 何 世界 変わる 魂 持つ 最後 サムライ 男 名 坂田銀時 いい加減 無鉄砲 キメる さりげなく キメる 笑える 泣ける 心温まる 銀さん 仲間達 生き様 得 ご覧\n",
      "968\n",
      "------------------------------------\n",
      "銀魂 (3年目)\n",
      "天人 宇宙人 来襲 価値観 変わる 町 江戸 宇宙人 高層ビル バイク 何 世界 変わる 魂 持つ 最後 サムライ 男 名 坂田銀時 いい加減 無鉄砲 キメる さりげなく キメる 笑える 泣ける 心温まる 銀さん 仲間達 生き様 得 ご覧\n",
      "969\n"
     ]
    }
   ],
   "source": [
    "recommend(\"のんのんびより\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}