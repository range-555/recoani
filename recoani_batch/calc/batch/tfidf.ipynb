{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import MeCab\n",
    "import mysql.connector as mydb\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mydb.connect(\n",
    "    host = \"localhost\",\n",
    "    port = 3306,\n",
    "    user = \"root\",\n",
    "    password = \"root\",\n",
    "    database = \"recoani\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "mecab = MeCab.Tagger()\n",
    "stop_word = []\n",
    "stop_word_regex = []\n",
    "vocab_list = []\n",
    "get_word = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    \n",
    "    def __init__(self,sentence):\n",
    "        self.sentence = sentence\n",
    "        \n",
    "    def normalize(self):\n",
    "        return self.normalize_neologd()\n",
    "\n",
    "    def unicode_normalize(self,cls,s):\n",
    "        pt = re.compile('([{}]+)'.format(cls))\n",
    "\n",
    "        def norm(c):\n",
    "            return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
    "    \n",
    "        s = ''.join(norm(x) for x in re.split(pt, s))\n",
    "        s = re.sub('－', '-', s)\n",
    "        return s\n",
    "\n",
    "    def remove_extra_spaces(self,s):\n",
    "        s = re.sub('[ 　]+', ' ', s)\n",
    "        blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
    "                          '\\u3040-\\u309F',  # HIRAGANA\n",
    "                          '\\u30A0-\\u30FF',  # KATAKANA\n",
    "                          '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
    "                          '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
    "                          ))\n",
    "        basic_latin = '\\u0000-\\u007F'\n",
    "    \n",
    "        def remove_space_between(cls1,cls2,s):\n",
    "            p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
    "            while p.search(s):\n",
    "                s = p.sub(r'\\1\\2', s)\n",
    "            return s\n",
    "    \n",
    "        s = remove_space_between(blocks, blocks, s)\n",
    "        s = remove_space_between(blocks, basic_latin, s)\n",
    "        s = remove_space_between(basic_latin, blocks, s)\n",
    "        return s\n",
    "    \n",
    "    def normalize_neologd(self):\n",
    "        s = self.sentence\n",
    "        s = s.strip()\n",
    "        s = self.unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
    "        def maketrans(f, t):\n",
    "            return {ord(x): ord(y) for x, y in zip(f, t)}\n",
    "    \n",
    "        s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
    "        s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
    "        s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
    "        s = s.translate(\n",
    "            maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
    "                  '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
    "        s = self.remove_extra_spaces(s)\n",
    "        s = self.unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
    "        s = re.sub('[’]', '\\'', s)\n",
    "        s = re.sub('[”]', '\"', s)\n",
    "        s = s.upper()\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素解析用クラス\n",
    "class Morph(object):\n",
    "    def __init__(self, surface, pos, base):\n",
    "        self.surface = surface\n",
    "        self.pos = pos\n",
    "        self.base = base\n",
    "    def __repr__(self):\n",
    "        return str({\n",
    "            \"surface\": self.surface,\n",
    "            \"pos\": self.pos,\n",
    "            \"base\": self.base\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence,title):\n",
    "    if len(sentence) < 10:\n",
    "        return []\n",
    "        \n",
    "    s = Normalize(sentence)\n",
    "    sentence = s.normalize()\n",
    "    mecab.parse(\"\")\n",
    "    lines = mecab.parse(sentence).split(\"\\n\")\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        elems = line.split(\"\\t\")\n",
    "        if len(elems) < 2:\n",
    "            continue\n",
    "        surface = elems[0]\n",
    "        if len(surface):\n",
    "            feature = elems[1].split(\",\")\n",
    "            base = surface if len(feature) < 7 or feature[6] == \"*\" else feature[6]\n",
    "            pos = \",\".join(feature[0:4])\n",
    "            tokens.append(Morph(surface=surface, pos=pos, base=base))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_list(data):\n",
    "    vocab = {}\n",
    "    for item in data:\n",
    "        tokens = item['tokens']\n",
    "        for token in tokens:\n",
    "            key = token.base\n",
    "            pos = token.pos\n",
    "            v = vocab.get(key, { \"count\": 0, \"pos\": pos })\n",
    "            v[\"count\"] += 1\n",
    "            vocab[key] = v\n",
    "    \n",
    "    for k in vocab:\n",
    "        v = vocab[k]\n",
    "        vocab_list.append((v[\"count\"], k, v[\"pos\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop(vocab):\n",
    "    return vocab[1] in stop_word or any([r for r in stop_word_regex if r.match(vocab[1]) is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_recommend_list(id, recommend_list):\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "          UPDATE animes\n",
    "          SET recommend_list = %(recommend_list)s\n",
    "          WHERE id = %(id)s\n",
    "          \"\"\"\n",
    "        cursor.execute(query, {'id': id, 'recommend_list': recommend_list})\n",
    "        connection.commit()\n",
    "    except Exception as e:\n",
    "        connection.rollback()\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor(buffered=True, dictionary=True)\n",
    "cursor.execute(\"SELECT id, title, outline_entire FROM animes\")\n",
    "data = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    item['tokens'] = tokenize(item['outline_entire'],item['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vocab_list(data)\n",
    "vocab_list = sorted(vocab_list, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [v for v in vocab_list if not (\"助詞\" in v[2] or \"記号\" in v[2] or \"助動詞\" in v[2] or \"接続詞\" in v[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word += ['する', 'れる','いる', 'ある', 'たち',\n",
    "    'ない', 'なる', '人', 'その', '(', '\"',\n",
    "    ')', '.', '/', 'ー','そして', '年', '中',\n",
    "    'そんな', '一','2', '二', 'それ', 'この',\n",
    "    '1', '3', '第', 'できる', 'させる']\n",
    "stop_word += [\n",
    "    '監督','アニメーション','メンバー','エピソード','スタッフ','時代',\n",
    "]\n",
    "stop_word_regex = [ re.compile(\"^[!?]+$\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word = [v[1] for v in vocab_list if v[0] > 3 and not is_stop(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = {'id': [],'title': [] ,'outline': []}\n",
    "\n",
    "for item in data:\n",
    "    items['id'].append(item['id'])\n",
    "    items['title'].append(item['title'])\n",
    "    base = []\n",
    "    for token in item[\"tokens\"]:\n",
    "        if token.base not in get_word:\n",
    "            continue\n",
    "        base.append(token.base)\n",
    "    items['outline'].append(' '.join(base))\n",
    "    if item['id']%100==0:\n",
    "        print(time.perf_counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=1,\n",
    "    max_features=1500,\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_fit = tfidf.fit(items['outline'])\n",
    "tfidf_transform = tfidf.transform(items['outline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(tfidf_transform, tfidf_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    id = item['id']\n",
    "    target_id = id - 1\n",
    "    sim_items_idx = cos_sim[target_id].argsort()[::-1][:35]\n",
    "    recommend_list = []\n",
    "    rank = 1\n",
    "    for idx in sim_items_idx[1:]:\n",
    "        if items['title'][idx]==item['title']:\n",
    "            continue\n",
    "        recommend_elm = {\"id\": items['id'][idx], \"sim\":cos_sim[target_id][idx], \"rank\": rank}\n",
    "        recommend_list.append(recommend_elm)\n",
    "        rank += 1\n",
    "    recommend_list = json.dumps(recommend_list)\n",
    "    update_recommend_list(id, recommend_list)\n",
    "    if id%100==0:\n",
    "        print(time.perf_counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT recommend_list FROM animes\")\n",
    "rec = cursor.fetchall()\n",
    "for i, r in enumerate(rec):\n",
    "    recommend_list = json.loads(r[\"recommend_list\"])\n",
    "    print(recommend_list)\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(target):\n",
    "    cursor.execute(\"SELECT id FROM animes where title = %s\", [target])\n",
    "    target_data = cursor.fetchone()\n",
    "    target_id = target_data[\"id\"] - 1\n",
    "    sim_items_idx = cos_sim[target_id].argsort()[::-1][:20]\n",
    "    print(\"タイトル \" + items['title'][target_id])\n",
    "    print(items['outline'][target_id])\n",
    "    print(\"id \" + str(items[\"id\"][target_id]))\n",
    "    for idx in sim_items_idx[1:]:\n",
    "        if items['title'][idx]==target:\n",
    "            continue\n",
    "        print('------------------------------------')\n",
    "        print(items['title'][idx])\n",
    "        print(items['outline'][idx])\n",
    "        print(items[\"id\"][idx])\n",
    "        print(cos_sim[target_id][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommend(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#以下失敗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docs = []\n",
    "for i, item in enumerate(data):\n",
    "    words = []\n",
    "    for token in item[\"tokens\"]:\n",
    "        if token.base not in get_word:\n",
    "            continue\n",
    "        words.append(token.surface)\n",
    "    docs.append(TaggedDocument(words=words, tags=[item[\"id\"], item[\"title\"]]))\n",
    "    time_in_loop = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Doc2Vec(docs, size=100,alpha=0.0005,min_alpha=0.000001,window=15, min_count=1)\n",
    "model.save(\"./doc2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m = model.docvecs.most_similar(5)\n",
    "for d in m:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(model.docvecs.similarity(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(docs[1],docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
