{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urllib2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-257-1f11a917db03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'urllib2'"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import MeCab\n",
    "import mysql.connector as mydb\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "import unicodedata\n",
    "import urllib2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mydb.connect(\n",
    "    host = \"localhost\",\n",
    "    port = 3306,\n",
    "    user = \"root\",\n",
    "    password = \"root\",\n",
    "    database = \"recoani\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "mecab = MeCab.Tagger()\n",
    "stop_word = []\n",
    "stop_word_regex = []\n",
    "vocab_list = []\n",
    "get_word = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    \n",
    "    def __init__(self,sentence):\n",
    "        self.sentence = sentence\n",
    "        \n",
    "    def normalize(self):\n",
    "        return self.normalize_neologd()\n",
    "\n",
    "    def unicode_normalize(self,cls,s):\n",
    "        pt = re.compile('([{}]+)'.format(cls))\n",
    "\n",
    "        def norm(c):\n",
    "            return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
    "    \n",
    "        s = ''.join(norm(x) for x in re.split(pt, s))\n",
    "        s = re.sub('－', '-', s)\n",
    "        return s\n",
    "\n",
    "    def remove_extra_spaces(self,s):\n",
    "        s = re.sub('[ 　]+', ' ', s)\n",
    "        blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
    "                          '\\u3040-\\u309F',  # HIRAGANA\n",
    "                          '\\u30A0-\\u30FF',  # KATAKANA\n",
    "                          '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
    "                          '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
    "                          ))\n",
    "        basic_latin = '\\u0000-\\u007F'\n",
    "    \n",
    "        def remove_space_between(cls1,cls2,s):\n",
    "            p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
    "            while p.search(s):\n",
    "                s = p.sub(r'\\1\\2', s)\n",
    "            return s\n",
    "    \n",
    "        s = remove_space_between(blocks, blocks, s)\n",
    "        s = remove_space_between(blocks, basic_latin, s)\n",
    "        s = remove_space_between(basic_latin, blocks, s)\n",
    "        return s\n",
    "    \n",
    "    def normalize_neologd(self):\n",
    "        s = self.sentence\n",
    "        s = s.strip()\n",
    "        s = self.unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
    "        def maketrans(f, t):\n",
    "            return {ord(x): ord(y) for x, y in zip(f, t)}\n",
    "    \n",
    "        s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
    "        s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
    "        s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
    "        s = s.translate(\n",
    "            maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
    "                  '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
    "        s = self.remove_extra_spaces(s)\n",
    "        s = self.unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
    "        s = re.sub('[’]', '\\'', s)\n",
    "        s = re.sub('[”]', '\"', s)\n",
    "        s = s.upper()\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素解析用クラス\n",
    "class Morph(object):\n",
    "    def __init__(self, surface, pos, base):\n",
    "        self.surface = surface\n",
    "        self.pos = pos\n",
    "        self.base = base\n",
    "    def __repr__(self):\n",
    "        return str({\n",
    "            \"surface\": self.surface,\n",
    "            \"pos\": self.pos,\n",
    "            \"base\": self.base\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence,title):\n",
    "    if len(sentence) < 10:\n",
    "        return []\n",
    "        \n",
    "    s = Normalize(sentence)\n",
    "    sentence = s.normalize()\n",
    "    mecab.parse(\"\")\n",
    "    lines = mecab.parse(sentence).split(\"\\n\")\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        elems = line.split(\"\\t\")\n",
    "        if len(elems) < 2:\n",
    "            continue\n",
    "        surface = elems[0]\n",
    "        if len(surface):\n",
    "            feature = elems[1].split(\",\")\n",
    "            base = surface if len(feature) < 7 or feature[6] == \"*\" else feature[6]\n",
    "            pos = \",\".join(feature[0:4])\n",
    "            tokens.append(Morph(surface=surface, pos=pos, base=base))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_list(data):\n",
    "    vocab = {}\n",
    "    for item in data:\n",
    "        tokens = item['tokens']\n",
    "        for token in tokens:\n",
    "            key = token.base\n",
    "            pos = token.pos\n",
    "            v = vocab.get(key, { \"count\": 0, \"pos\": pos })\n",
    "            v[\"count\"] += 1\n",
    "            vocab[key] = v\n",
    "    \n",
    "    for k in vocab:\n",
    "        v = vocab[k]\n",
    "        vocab_list.append((v[\"count\"], k, v[\"pos\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop(vocab):\n",
    "    return vocab[1] in stop_word or any([r for r in stop_word_regex if r.match(vocab[1]) is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor(buffered=True, dictionary=True)\n",
    "cursor.execute(\"SELECT id, title, outline_entire FROM animes\")\n",
    "data = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    item['tokens'] = tokenize(item['outline_entire'],item['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vocab_list(data)\n",
    "vocab_list = sorted(vocab_list, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [v for v in vocab_list if not (\"助詞\" in v[2] or \"記号\" in v[2] or \"助動詞\" in v[2] or \"接続詞\" in v[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urllib2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-3bbf3f0d6b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mslothlib_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mslothlib_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslothlib_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mslothlib_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslothlib_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mslothlib_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslothlib_stopwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34mu''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'urllib2' is not defined"
     ]
    }
   ],
   "source": [
    "slothlib_path = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "slothlib_file = urllib2.urlopen(slothlib_path)\n",
    "slothlib_stopwords = [line.decode(\"utf-8\").strip() for line in slothlib_file]\n",
    "slothlib_stopwords = [ss for ss in slothlib_stopwords if not ss==u'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word += ['する', 'れる','いる', 'ある', 'たち',\n",
    "    'ない', 'なる', '人', 'その', '(', '\"',\n",
    "    ')', '.', '/', 'ー','そして', '年', '中',\n",
    "    'そんな', '一','2', '二', 'それ', 'この',\n",
    "    '1', '3', '第', 'できる', 'させる']\n",
    "stop_word += [\n",
    "    '監督','アニメーション','メンバー','エピソード','スタッフ','時代',\n",
    "]\n",
    "stop_word += sto\n",
    "stop_word_regex = [ re.compile(\"^[!?]+$\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word = [v[1] for v in vocab_list if v[0] > 3 and not is_stop(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9305.499255057\n",
      "9307.424731931\n",
      "9309.860942409\n",
      "9311.814449856\n",
      "9313.992755705\n",
      "9316.107269936\n",
      "9318.167946111\n",
      "9321.076891524\n",
      "9324.268809367\n",
      "9327.077926867\n",
      "9329.519492783\n",
      "9331.653991657\n",
      "9333.483343676\n",
      "9335.71304301\n",
      "9337.95913415\n",
      "9340.193874457\n",
      "9342.356021083\n",
      "9345.261991708\n",
      "9347.59662081\n",
      "9349.777880368\n",
      "9351.55700531\n",
      "9354.284571487\n",
      "9356.488963381\n",
      "9358.530259494\n",
      "9360.39048928\n",
      "9362.31259768\n",
      "9364.365469399\n",
      "9366.427404899\n",
      "9368.682366559\n",
      "9370.868122921\n",
      "9372.951814258\n",
      "9375.570904038\n",
      "9377.763483175\n",
      "9379.776137854\n",
      "9381.776902671\n",
      "9383.827019336\n",
      "9386.193184699\n",
      "9388.633749883\n",
      "9390.605043802\n"
     ]
    }
   ],
   "source": [
    "items = {'id': [],'title': [] ,'outline': []}\n",
    "\n",
    "for item in data:\n",
    "    items['id'].append(item['id'])\n",
    "    items['title'].append(item['title'])\n",
    "    base = []\n",
    "    for token in item[\"tokens\"]:\n",
    "        if token.base not in get_word:\n",
    "            continue\n",
    "        base.append(token.base)\n",
    "    items['outline'].append(' '.join(base))\n",
    "    if item['id']%100==0:\n",
    "        print(time.perf_counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=1,\n",
    "    max_features=1500,\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_fit = tfidf.fit(items['outline'])\n",
    "tfidf_transform = tfidf.transform(items['outline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(tfidf_transform, tfidf_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.4924357264020337\n",
      "ああっ女神さまっ　それぞれの翼\n",
      "3\n",
      "0.32416933887531013\n",
      "ああっ女神さまっ　闘う翼\n",
      "3368\n",
      "0.2693672598942226\n",
      "みつどもえ　増量中！\n",
      "2254\n",
      "0.260548146707484\n",
      "同級生２Special 卒業生\n",
      "3169\n",
      "0.23666365086781244\n",
      "xxxHOLiC◆継\n",
      "2650\n",
      "0.21725149963110596\n",
      "バトルスピリッツ ブレイヴ\n",
      "1251\n",
      "0.21697017632828425\n",
      "最遊記外伝\n",
      "671\n",
      "0.21328987177160913\n",
      "劇場版「空の境界」終章\n",
      "2632\n",
      "0.21246199723728926\n",
      "元祖 爆れつハンター\n",
      "537\n",
      "0.2120755920068079\n",
      "おねがいマイメロディ きららっ☆\n",
      "1246\n",
      "0.20784604690947306\n",
      "最終兵器彼女　Another love song\n",
      "1982\n",
      "0.19591932708978288\n",
      "超次元ゲイム ネプテューヌ\n",
      "3349\n",
      "0.19186287141024183\n",
      "まんがーる！\n",
      "1250\n",
      "0.19122457359517828\n",
      "｢最遊記RELOAD-burial-｣\n",
      "1381\n",
      "0.1902853859932571\n",
      "灼眼のシャナS(OVA)\n",
      "1926\n",
      "0.18638658707472217\n",
      "ダンジョンに出会いを求めるのは間違っているだろうかⅡ\n",
      "3760\n",
      "0.18400703047369207\n",
      "LEMON ANGEL PROJECT\n",
      "2045\n",
      "0.1839304301948006\n",
      "テイルズ オブ エターニア　THE ANIMATION\n",
      "2463\n",
      "0.18293878234983224\n",
      "乃木坂春香の秘密\n",
      "3511\n",
      "0.18096763708179048\n",
      "ヤマノススメ\n",
      "3426\n",
      "0.17968314681750125\n",
      "無責任艦長タイラー\n",
      "2062\n",
      "0.17891416676983285\n",
      "鉄人28号\n",
      "2478\n",
      "0.1777440392721915\n",
      "野良スコ\n",
      "2224\n",
      "0.17478578221050559\n",
      "To LOVEる-とらぶる- ダークネス2nd\n",
      "2845\n",
      "0.16756832503882943\n",
      "ファイブスター物語\n",
      "3695\n",
      "0.16521625544635554\n",
      "ルパン三世 PARTⅢ\n",
      "1095\n",
      "0.16423996179691952\n",
      "月曜日のたわわ\n",
      "3298\n",
      "0.16324110835066247\n",
      "劇場版 魔法少女まどか☆マギカ［前編］始まりの物語\n",
      "3299\n",
      "0.16324110835066247\n",
      "劇場版 魔法少女まどか☆マギカ［後編］永遠の物語\n",
      "664\n",
      "0.16287240148801294\n",
      "神のみぞ知るセカイ 女神篇\n",
      "1832\n",
      "0.1614421654372816\n",
      "逮捕しちゃうぞ SECOND SEASON\n",
      "313\n",
      "0.1570835512861053\n",
      "EAT-MAN\n",
      "558\n",
      "0.15523586297876626\n",
      "「俺の妹がこんなに可愛いわけがない。」TV未放送 第14話～最終回第16話\n",
      "557\n",
      "0.15523586297876626\n",
      "俺の妹がこんなに可愛いわけがない。\n"
     ]
    }
   ],
   "source": [
    "for item in data:\n",
    "    id = item['id']\n",
    "    target_id = id - 1\n",
    "    sim_items_idx = cos_sim[target_id].argsort()[::-1][:35]\n",
    "    for idx in sim_items_idx[1:]:\n",
    "        if items['title'][idx]==item['title']:\n",
    "            continue\n",
    "        print(items['id'][idx])\n",
    "        print(cos_sim[target_id][idx])\n",
    "        print(items['title'][idx])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(target):\n",
    "    cursor.execute(\"SELECT id FROM animes where title = %s\", [target])\n",
    "    target_data = cursor.fetchone()\n",
    "    target_id = target_data[\"id\"] - 1\n",
    "    sim_items_idx = cos_sim[target_id].argsort()[::-1][:20]\n",
    "    print(\"タイトル \" + items['title'][target_id])\n",
    "    print(items['outline'][target_id])\n",
    "    print(\"id \" + str(items[\"id\"][target_id]))\n",
    "    for idx in sim_items_idx[1:]:\n",
    "        if items['title'][idx]==target:\n",
    "            continue\n",
    "        print('------------------------------------')\n",
    "        print(items['title'][idx])\n",
    "        print(items['outline'][idx])\n",
    "        print(items[\"id\"][idx])\n",
    "        print(cos_sim[target_id][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "喰霊-零-\n",
      "タイトル 喰霊-零-\n",
      "憎しみ 揺れる 魂 目覚める させる 誰 闇 街 彷徨 哀しみ 暮れる 運命 絆 繋ぐ 宿命 鎖 断ち切る 悪霊 討つ\n",
      "id 733\n",
      "------------------------------------\n",
      "バジリスク ～桜花忍法帖～\n",
      "忍法 殺戮 合戦 世 起きる 三 代 将軍 世継ぎ 争い 甲賀 伊賀 忍法 殺戮 合戦 凄惨 極める 散る 行く 愛 生きる 男 女 また 儚い 美しい 散る しかし ところ 服部 半蔵 響 八郎 手 つかの間 枚 生 与える られる 忘れ形見 また 残す いう 甲賀 八郎 伊賀 響 それぞれ 父 母 良い 似る 眼 持つ 生まれる 宿命 子供 時 世 忍び 行く末 行く 血 力 なんとか もの ん 甲賀 伊賀 者 達 八郎 響 こと 望む 契り 実 兄妹 契り 結ぶ 宿命 背負う す 宿命 翻弄 迷う 生きる 八郎 己 深い 愛 生きる 響 織り成す 生 先 未知 現象 生まれる 乱世 影\n",
      "2637\n",
      "0.2639681747205098\n",
      "------------------------------------\n",
      "Butlers～千年百年物語～\n",
      "とある 宿命 背負う 千 歴史 もつ 一族 守護 者 ジェイ 妹 同じ バトラー 羽 平和 穏やか 生活 送る しかし 平穏 日々 一変 時空 歪み 飲み込む ジェイ 身 100 後 世界 目覚める ジェイ 目 もの かつて 暮らす 屋敷 京 学園 ジェイ 神 高 馬 名乗る 学園 生徒 会長 過去 手がかり 探す すべて かつて 日々 取り戻す ため 百 想い 寄せる 再会 決別 歴史 千 秘密 宿命 ジェイ 世界 真実 知る 百 思い 今 交錯\n",
      "2644\n",
      "0.2476269888532256\n",
      "------------------------------------\n",
      "ご注文はうさぎですか？？ ～Sing For You～\n",
      "音楽 会 ソロ 選ぶ チノ 特訓 お願い リゼ 人前 歌う こと 慣れる させる 大会 開催 チノ 練習 いう みんな 盛り上がる しまう 緊張 チノ 当日 ちゃんと\n",
      "1222\n",
      "0.2237304769638736\n",
      "------------------------------------\n",
      "宇宙エース\n",
      "世界 的 科学 者 博士 娘 海底 光る 調査 宇宙 乗る カプセル 発見 偶然 目覚める させる しまう 宇宙 少年 星 少年 エース エース 空気 エネルギー 一瞬 集める 不思議 輪 シルバー リング 作り出す シルバー リング 乗る 怪 ロボット 悪 宇宙 相手 地球 宇宙 大 活躍\n",
      "353\n",
      "0.20296931404730245\n",
      "------------------------------------\n",
      "劇場版　巨人の星　宿命の対決\n",
      "魔球 大リーグ ボール 完成 せる 飛 雄 馬 宿命 ライバル 花形 満 対決 つまり 伝統 巨人 対 阪神 戦 勝負 行方 飛 雄 馬 大リーグ ボール 運命 また 最後 恐ろしい 事 起こる 川上 予言 いったい\n",
      "876\n",
      "0.20086543261689777\n",
      "------------------------------------\n",
      "聖剣使いの禁呪詠唱（ワールドブレイク）\n",
      "私立 亜 鐘 学園 高校 そこ 前世 記憶 目覚める 若者 救世主 )」 集う 学び舎 者 前世 記憶 もと 自ら 身体 )》 武器 体 術 戦 技 もつ 敵 砕く 白 鉄 )」 また 者 物理 越える 異 能 魔力 )》 自在 操る この世 魔術 業 敵 滅ぼす 黒 魔 くろい ま )」 亜 鐘 学園 少年 入学 彼 名 灰 村 葉 史上 初めて 白 鉄 黒 魔 二つ 前世 剣 聖 術 保持 者 力 目覚める 彼 それぞれ 前世 永遠 絆 結ぶ 最愛 少女 同時に 再会 果たす 誰 特別 運命 歩む 始める\n",
      "1673\n",
      "0.189386177169908\n",
      "------------------------------------\n",
      "劇場版　巨人の星　大リーグボール\n",
      "飛 雄 馬 栄光 巨人軍 入団 しかし 喜び つかの間 致命 的 弱点 球 さ 軍 屈辱 軍 生活 遂に 大リーグ ボール 号 完成 せる 宿命 ライバル 花形 満 左門 豊作 対決 行方\n",
      "875\n",
      "0.1807631362336\n",
      "------------------------------------\n",
      "パンツァードラグーン\n",
      "かつて 世界 震撼 せる 旧 世紀 塔 目覚める 最終 プログラム 発動 遺跡 化す 生物 兵器 工場 ドラゴン 長い 眠り 目覚める 辺境 地 住む 攻 性 生物 ハンター 少女 前 黒い 飛 竜 迫る 少女 連れ去る しまう 青い 飛 竜 力 借りる 助け出す 冒険 出る\n",
      "2711\n",
      "0.1768262803361292\n",
      "------------------------------------\n",
      "エスパー魔美【デジタルリマスター版】\n",
      "郊外 中学校 通う 普通 少女 佐倉 魔 美 日 ピンチ 陥る クラスメート 君 助ける 際 超 能力 目覚める 焼き 彼女 目覚める 能力 人助け 使う 決心 ただ ひとり 秘密 共有 君 協力 仰ぐ 様々 事件 対応 いく 時には 失敗 めぐる 合う 人々 交流 多く 経験 積み重ねる 人間 的 成長 いく ハートフルストーリー\n",
      "410\n",
      "0.17528371014689612\n",
      "------------------------------------\n",
      "ちょこッとSister\n",
      "クリスマス 朝 主人公 はる ま 妹 ちょこ もらう こと はる ま 実は 子供 頃 妹 欲しい 叶う ため 妹 ほしい お願い 今 実現 ちょこ 服 買う ため デパート 赴く はる ま ちょこ しかし はる ま 買い物 目 離す すき ちょこ デパート 迷子 しまう 迷子 所 ちょこ 彼女 自分 頼り お 兄ちゃん 思い ほのか 芽生える させる\n",
      "1997\n",
      "0.1722672090697474\n",
      "------------------------------------\n",
      "舞台『DIABOLIK LOVERS～re:requiem～』\n",
      "美しい 咲き乱れる 薔薇 香り 少女 己 運命 知る 運命 糸 導く 少女 彼ら 出会う 初めて オレ 様 すべて 奪う やる 僕 こと 嫌い もっと 知る ?」「 痛い もっと 痛い ちゃん あんた 家 来る 理由 わかる 極 血 人間 単なる 血 器 すぎる ん オレ 関わる 以上 近づく ぶっ 壊す やる !」 彼ら 血 欲望 ヴァンパイア 彼女 何 狂う 始める 何 目覚める いく もう どこ 逃れる こと 出来る\n",
      "2106\n",
      "0.17042188385474183\n",
      "------------------------------------\n",
      "DIABOLIK LOVERS\n",
      "美しい 咲き乱れる 薔薇 香り 少女 己 運命 知る 運命 糸 導く 少女 彼ら 出会う 初めて オレ 様 すべて 奪う やる 僕 こと 嫌い もっと 知る ?」「 痛い もっと 痛い ちゃん あんた 家 来る 理由 わかる 極 血 人間 単なる 血 器 すぎる ん オレ 関わる 以上 近づく ぶっ 壊す やる !」 彼ら 血 欲望 ヴァンパイア 彼女 何 狂う 始める 何 目覚める ゆく もう どこ 逃れる こと 出来る\n",
      "2102\n",
      "0.1692694332059277\n",
      "------------------------------------\n",
      "フルーツバスケット 2nd season \n",
      "透 紫 呉 家 住む 始める 経つ 由希 草 摩 家 皆 交流 深める くる 今 気 忌まわしい 呪い 正体 進む 道 決める られる 宿命 終わる 十二支 宴 前 由希 透 何 想う 何 決意\n",
      "2967\n",
      "0.16862191173871563\n",
      "------------------------------------\n",
      "三国志\n",
      "曹操 中原 覇 唱える 戦い 邁進 人々 暮らし 平安 守る 戦う 将 名 劉 備 彼 彼 義兄弟 契り 結ぶ 将 関 羽 張 飛 平和 国 作る 戦い 臨む だが 圧倒的 曹操 武力 前 何 度 させる られる 必要 劉 備 孔明 礼 もつ 迎える 知恵 得る 呉 一気に 覇者 ん 曹操 阻む 呉 国 孫 権 劉 備 孫 権 妹 麗 花 ロマンス 最大 赤 壁 戦い 向かう 物語 加速\n",
      "1335\n",
      "0.16583644954475169\n",
      "------------------------------------\n",
      "グリムノーツ The Animation\n",
      "人々 生まれる とき 冊 本 与える られる 生まれる 死ぬ 運命 書 脚本 通り 生きる 宿命 脚本 空白 書 もつ 生まれる 同じ 空白 書 もつ 仲間 レイナ 出会う 自ら 役割 見つける 旅 出る これ 繰り返す 読む 物語\n",
      "1040\n",
      "0.16314407345650409\n",
      "------------------------------------\n",
      "シャーマンキング\n",
      "夜 墓場 ま ん 太 出会う 少年 葉 森 羅 学園 転校生 やって来る 彼 この世 結ぶ 者 シャー マン 600 前 サムライ 霊 丸 運命 的 出会い 葉 シャー マン 王 : ため 資質 開花 せる いく 宿命 ライバル 道 蓮 はじめ 世 浄化 シャー マン 王 目指す 様々 ライバル 登場 巨大 彗星 夜空 駆ける 世界中 シャー マン 達 座 巡る 闘い 始まり 告げる 今 運命 歯車 回る はじめる\n",
      "1390\n",
      "0.16173305419619904\n",
      "------------------------------------\n",
      "政宗くんのリベンジ \n",
      "復讐 ため 俺 町 帰る くる 8 前 美少女 安達 愛 姫 フラ 冴える 少年 真壁 政 宗 変える イケメン 変身 帰る 来る すべて 残虐 姫 異名 持つ ド S 彼女 惚れる させる 最高 形 振る 復讐 ため\n",
      "3244\n",
      "0.16127205018603283\n",
      "------------------------------------\n",
      "えむえむっ！\n",
      "女性 しまう 体質 目覚める しまう 砂 太郎 ド M )。 まま 普通 恋 はず 体質 治す ため 生徒 願い 叶える くれる 部 訪れる そこ 自称 神 名乗る 激しい 勘違い 美少女 石動 ド S 太郎 ド M 目覚める きっかけ 結 野 嵐 子 性格 不明 --。 女の子 放つ 試練 快楽 数々 がんばる 太郎 そこ 喜ぶ ダメ ん\n",
      "423\n",
      "0.15859885362596102\n",
      "------------------------------------\n",
      "マジカノ\n",
      "地味 平凡 中学生 春生 主人公 三 妹 暮らす 春生 日 転校生 超 絶世 美女 魔 宮 あゆみ メイド やってくる 実は あゆみ 魔女 呪い 解く ため 春生 魔力 必要 いう 春生 魔力 目覚める させる 画策 あゆみ 野望 阻止 ため 春生 三 妹 立ちはだかる 平穏 春生 日常 日々 変貌 いく あゆみ かける られる 呪い 解く 呪い 元凶 春生 あゆみ 過去 因縁 何 次第に 色々 事実 明らか いく\n",
      "3251\n",
      "0.1555965234948966\n"
     ]
    }
   ],
   "source": [
    "recommend(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#以下失敗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docs = []\n",
    "for i, item in enumerate(data):\n",
    "    words = []\n",
    "    for token in item[\"tokens\"]:\n",
    "        if token.base not in get_word:\n",
    "            continue\n",
    "        words.append(token.surface)\n",
    "    docs.append(TaggedDocument(words=words, tags=[item[\"id\"], item[\"title\"]]))\n",
    "    time_in_loop = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Doc2Vec(docs, size=100,alpha=0.0005,min_alpha=0.000001,window=15, min_count=1)\n",
    "model.save(\"./doc2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m = model.docvecs.most_similar(5)\n",
    "for d in m:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(model.docvecs.similarity(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(docs[1],docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
